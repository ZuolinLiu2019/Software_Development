{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Analysis\n",
    "2. Feature Engineering\n",
    "3. Feature Selection\n",
    "4. Model Building\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Data Source: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "\n",
    "This dataset contains 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa.\n",
    "The goal is to predict the house sale price. \n",
    "\n",
    "In this notebook, we will proceed feature selection to best infromative feature for the training the model. This will help reduce the reduncy and noisy of the data as well as improve the model performance. \n",
    "\n",
    "We will select variables using the Lasso regression: Lasso has the property of setting the coefficient of non-informative variables to zero. This way we can identify those variables and remove them from our final models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for model \n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# to visualise al the columns in the dataframe\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# We  load the datasets with the engineered values: we built and saved these datasets in the previous lecture.\n",
    "# If you haven't done so, go ahead and check the previous lecture / notebook to find out how to create these datasets\n",
    "\n",
    "X_train = pd.read_csv('xtrain.csv')\n",
    "X_test = pd.read_csv('xtest.csv')\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# capture the target\n",
    "y_train = X_train['SalePrice']\n",
    "y_test = X_test['SalePrice']\n",
    "\n",
    "# drop unnecessary variables from our training and testing sets\n",
    "X_train.drop(['Id', 'SalePrice'], axis=1, inplace=True)\n",
    "X_test.drop(['Id', 'SalePrice'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here I will do the model fitting and feature selection\n",
    "# altogether in one line of code\n",
    "\n",
    "# first, I specify the Lasso Regression model, and I\n",
    "# select a suitable alpha (equivalent of penalty).\n",
    "# The bigger the alpha the less features that will be selected.\n",
    "\n",
    "# Then I use the selectFromModel object from sklearn, which\n",
    "# will select the features which coefficients are non-zero\n",
    "\n",
    "sel_ = SelectFromModel(Lasso(alpha=0.005, random_state=0)) # remember to set the seed, the random state in this function\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this command let's us visualise those features that were kept.\n",
    "# Kept features have a True indicator\n",
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's print the number of total and selected features\n",
    "\n",
    "# this is how we can make a list of the selected features\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "# let's print some stats\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "    np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the selected features\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is an alternative way of identifying the selected features \n",
    "# based on the non-zero regularisation coefficients:\n",
    "selected_feats = X_train.columns[(sel_.estimator_.coef_ != 0).ravel().tolist()]\n",
    "selected_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we save the selected list of features\n",
    "pd.Series(selected_feats).to_csv('selected_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
